# ä¼˜åŒ–å™¨é…ç½®æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬é¡¹ç›®ç°åœ¨æ”¯æŒä¸‰ç§ä¼˜åŒ–å™¨ï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°çµæ´»é…ç½®ï¼š
- **Adam**ï¼ˆé»˜è®¤ï¼‰ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæ”¶æ•›å¿«
- **SGD**ï¼šå¸¦åŠ¨é‡çš„éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæ³›åŒ–èƒ½åŠ›å¼º
- **AdamW**ï¼šAdam çš„æ”¹è¿›ç‰ˆï¼Œæƒé‡è¡°å‡å®ç°æ›´å¥½

**é‡è¦ç‰¹æ€§ï¼š** æ‰€æœ‰ç®—æ³•ï¼ˆFedAvgã€FedProxã€FedGenï¼‰ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰

```bash
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer adam
```

### ä½¿ç”¨ SGD ä¼˜åŒ–å™¨ï¼ˆå¸¦åŠ¨é‡ï¼‰

```bash
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer sgd --momentum 0.9
```

### ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨

```bash
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer adamw
```

---

## ğŸ“‹ å‘½ä»¤è¡Œå‚æ•°è¯¦è§£

### ä¼˜åŒ–å™¨ç›¸å…³å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--optimizer` | str | `adam` | ä¼˜åŒ–å™¨ç±»å‹ï¼ˆsgd, adam, adamwï¼‰ |
| `--learning_rate` | float | `0.01` | å­¦ä¹ ç‡ |
| `--momentum` | float | `0.9` | SGD åŠ¨é‡å‚æ•°ï¼ˆä»… sgd ä½¿ç”¨ï¼‰ |
| `--weight_decay` | float | `1e-4` | æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰ |

---

## ğŸ¨ ä¼˜åŒ–å™¨ç‰¹ç‚¹å¯¹æ¯”

### Adamï¼ˆé»˜è®¤ï¼‰

**ç‰¹ç‚¹ï¼š**
- âœ… è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œä¸éœ€è¦ç²¾å¿ƒè°ƒå‚
- âœ… æ”¶æ•›é€Ÿåº¦å¿«ï¼Œé€šå¸¸éœ€è¦æ›´å°‘çš„è®­ç»ƒè½®æ¬¡
- âœ… å¯¹åˆå§‹å­¦ä¹ ç‡ä¸æ•æ„Ÿ
- âš ï¸ åœ¨æŸäº›ä»»åŠ¡ä¸Šæ³›åŒ–èƒ½åŠ›ç•¥å¼±äº SGD

**æ¨èåœºæ™¯ï¼š**
- å¿«é€Ÿå®éªŒå’ŒåŸå‹å¼€å‘
- å¤æ‚æ¨¡å‹ï¼ˆå¦‚ ResNet1Dï¼‰
- æ—¶é—´æœ‰é™æ—¶

**æ¨èå‚æ•°ï¼š**
```bash
--optimizer adam --learning_rate 0.01 --weight_decay 1e-4
```

### SGDï¼ˆå¸¦åŠ¨é‡ï¼‰

**ç‰¹ç‚¹ï¼š**
- âœ… æ³›åŒ–èƒ½åŠ›å¼ºï¼Œæœ€ç»ˆæ¨¡å‹æ€§èƒ½é€šå¸¸æ›´å¥½
- âœ… è”é‚¦å­¦ä¹ è®ºæ–‡ä¸­æœ€å¸¸ç”¨
- âš ï¸ éœ€è¦ä»”ç»†è°ƒæ•´å­¦ä¹ ç‡
- âš ï¸ æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢

**æ¨èåœºæ™¯ï¼š**
- è¿½æ±‚æœ€ä½³æ€§èƒ½
- å¤ç°è®ºæ–‡å®éªŒ
- æœ‰å……è¶³æ—¶é—´è°ƒå‚

**æ¨èå‚æ•°ï¼š**
```bash
--optimizer sgd --learning_rate 0.01 --momentum 0.9 --weight_decay 1e-4
```

### AdamW

**ç‰¹ç‚¹ï¼š**
- âœ… Adam çš„æ”¹è¿›ç‰ˆæœ¬
- âœ… æ›´å¥½çš„æƒé‡è¡°å‡å®ç°
- âœ… åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šä¼˜äº Adam
- âœ… è‡ªé€‚åº”å­¦ä¹ ç‡

**æ¨èåœºæ™¯ï¼š**
- è¿½æ±‚ç°ä»£æœ€ä½³å®è·µ
- è¾ƒå¤§çš„æ¨¡å‹
- éœ€è¦æƒé‡è¡°å‡æ­£åˆ™åŒ–

**æ¨èå‚æ•°ï¼š**
```bash
--optimizer adamw --learning_rate 0.001 --weight_decay 0.01
```

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### 1. ç®—æ³•å¯¹æ¯”å®éªŒï¼ˆæ¨èï¼‰

**ç›®æ ‡ï¼š** å…¬å¹³å¯¹æ¯” FedAvgã€FedProxã€FedGen

**æ–¹æ¡ˆï¼š** æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨

```bash
# ä½¿ç”¨ Adamï¼ˆé»˜è®¤ï¼‰å¯¹æ¯”ä¸‰ç§ç®—æ³•
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer adam --num_rounds 100
python main.py --dataset RML2016.10a --algorithm FedProx --optimizer adam --num_rounds 100
python main.py --dataset RML2016.10a --algorithm FedGen --optimizer adam --num_rounds 100

# ç”Ÿæˆå¯¹æ¯”å›¾
python main_plot.py --dataset RML2016.10a --algorithms FedAvg FedProx FedGen
```

### 2. ä¼˜åŒ–å™¨å¯¹æ¯”å®éªŒ

**ç›®æ ‡ï¼š** æ‰¾å‡ºæœ€é€‚åˆè¯¥ä»»åŠ¡çš„ä¼˜åŒ–å™¨

**æ–¹æ¡ˆï¼š** å›ºå®šç®—æ³•ï¼Œæµ‹è¯•ä¸åŒä¼˜åŒ–å™¨

```bash
# ä½¿ç”¨ FedAvg ç®—æ³•ï¼Œæµ‹è¯•ä¸‰ç§ä¼˜åŒ–å™¨
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer adam --num_rounds 100
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer sgd --num_rounds 100
python main.py --dataset RML2016.10a --algorithm FedAvg --optimizer adamw --num_rounds 100
```

### 3. è¶…å‚æ•°è°ƒä¼˜

**Adam å­¦ä¹ ç‡èŒƒå›´ï¼š** 0.001 - 0.01
```bash
python main.py --optimizer adam --learning_rate 0.001
python main.py --optimizer adam --learning_rate 0.005
python main.py --optimizer adam --learning_rate 0.01
```

**SGD å­¦ä¹ ç‡èŒƒå›´ï¼š** 0.01 - 0.1
```bash
python main.py --optimizer sgd --learning_rate 0.01 --momentum 0.9
python main.py --optimizer sgd --learning_rate 0.05 --momentum 0.9
python main.py --optimizer sgd --learning_rate 0.1 --momentum 0.9
```

**æƒé‡è¡°å‡èŒƒå›´ï¼š** 1e-5 - 1e-3
```bash
python main.py --optimizer adam --weight_decay 1e-5
python main.py --optimizer adam --weight_decay 1e-4
python main.py --optimizer adam --weight_decay 1e-3
```

---

## ğŸ“Š é¢„æœŸæ€§èƒ½å·®å¼‚

åŸºäº RML2016.10a æ•°æ®é›†ï¼ˆ100è½®è®­ç»ƒï¼‰ï¼š

| ä¼˜åŒ–å™¨ | æ”¶æ•›é€Ÿåº¦ | æœ€ç»ˆå‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ |
|--------|---------|-----------|---------|
| Adam | â­â­â­â­ | 55-65% | åŸºå‡† |
| SGD | â­â­â­ | 56-66% | åŸºå‡† x 1.2 |
| AdamW | â­â­â­â­ | 56-66% | åŸºå‡† |

**æ³¨æ„ï¼š** å®é™…ç»“æœå—æ•°æ®é›†ã€æ¨¡å‹ã€éšæœºç§å­ç­‰å› ç´ å½±å“ã€‚

---

## ğŸ” æ—¥å¿—ä¸­çš„ä¼˜åŒ–å™¨ä¿¡æ¯

è®­ç»ƒæ—¶ï¼Œæ—¥å¿—ä¼šæ˜¾ç¤ºä¼˜åŒ–å™¨é…ç½®ï¼š

```
================================================================================
è”é‚¦å­¦ä¹ è‡ªåŠ¨è°ƒåˆ¶è¯†åˆ«
================================================================================
æ•°æ®é›†: RML2016.10a
ç®—æ³•: FedAvg
æ¨¡å‹: CNN1D
...
å­¦ä¹ ç‡: 0.01
ä¼˜åŒ–å™¨: ADAM
  - æƒé‡è¡°å‡: 0.0001
Non-IID ç±»å‹: iid
è®¾å¤‡: cuda
================================================================================
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: åº”è¯¥é€‰æ‹©å“ªä¸ªä¼˜åŒ–å™¨ï¼Ÿ

**A:** 
- **å¿«é€Ÿå®éªŒ/é»˜è®¤é€‰æ‹©ï¼š** Adam
- **è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼š** SGDï¼ˆå¸¦åŠ¨é‡ï¼‰
- **ç°ä»£æœ€ä½³å®è·µï¼š** AdamW

### Q2: ä¸ºä»€ä¹ˆæ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨ï¼Ÿ

**A:** ä¸ºäº†ç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚å¦‚æœä¸åŒç®—æ³•ä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨ï¼Œæ— æ³•åˆ¤æ–­æ€§èƒ½å·®å¼‚æ¥è‡ªç®—æ³•æœ¬èº«è¿˜æ˜¯ä¼˜åŒ–å™¨ã€‚

### Q3: FedGen çš„ç”Ÿæˆå™¨ä½¿ç”¨ä»€ä¹ˆä¼˜åŒ–å™¨ï¼Ÿ

**A:** ç”Ÿæˆå™¨å§‹ç»ˆä½¿ç”¨ Adamï¼ˆç”Ÿæˆæ¨¡å‹çš„æ ‡å‡†åšæ³•ï¼‰ï¼Œä¸å— `--optimizer` å‚æ•°å½±å“ã€‚åªæœ‰åˆ†ç±»å™¨ä½¿ç”¨æŒ‡å®šçš„ä¼˜åŒ–å™¨ã€‚

### Q4: å¦‚ä½•çŸ¥é“æˆ‘çš„æ¨¡å‹ä½¿ç”¨äº†å“ªä¸ªä¼˜åŒ–å™¨ï¼Ÿ

**A:** æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼ˆTXT æ–‡ä»¶ï¼‰çš„å¼€å¤´éƒ¨åˆ†ï¼Œä¼šæ˜¾ç¤ºä¼˜åŒ–å™¨ä¿¡æ¯ã€‚

---

## ğŸ“ è¿›é˜¶æŠ€å·§

### å­¦ä¹ ç‡è°ƒåº¦

è™½ç„¶å½“å‰ç‰ˆæœ¬ä¸åŒ…å«å­¦ä¹ ç‡è°ƒåº¦ï¼Œä½†å¯ä»¥é€šè¿‡å¤šæ¬¡è¿è¡Œå®ç°ï¼š

```bash
# é˜¶æ®µ1ï¼šé«˜å­¦ä¹ ç‡å¿«é€Ÿä¸‹é™
python main.py --optimizer adam --learning_rate 0.01 --num_rounds 50

# é˜¶æ®µ2ï¼šä½å­¦ä¹ ç‡ç²¾è°ƒï¼ˆéœ€è¦æ‰‹åŠ¨åŠ è½½æ¨¡å‹ï¼‰
python main.py --optimizer adam --learning_rate 0.001 --num_rounds 50
```

### æ··åˆç­–ç•¥

ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨ï¼š

```bash
# é˜¶æ®µ1ï¼šAdam å¿«é€Ÿæ”¶æ•›
python main.py --optimizer adam --learning_rate 0.01 --num_rounds 70

# é˜¶æ®µ2ï¼šSGD æå‡æ³›åŒ–ï¼ˆéœ€è¦å®ç°æ¨¡å‹åŠ è½½åŠŸèƒ½ï¼‰
python main.py --optimizer sgd --learning_rate 0.001 --num_rounds 30
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- Adam: Kingma & Ba (2014) - "Adam: A Method for Stochastic Optimization"
- SGD with Momentum: Polyak (1964) - "Some methods of speeding up convergence"
- AdamW: Loshchilov & Hutter (2017) - "Decoupled Weight Decay Regularization"

---

**äº«å—çµæ´»çš„ä¼˜åŒ–å™¨é…ç½®ï¼Good Luck! ğŸš€**

